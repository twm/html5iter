from typing import Optional, MutableMapping
from collections import abc

import attr

from .constants import tokenTypes


class Token(abc.MutableMapping):
    """
    Base class for tokens.

    As a temporary compatibility hack, this quacks like a token dict.
    """

    def __getitem__(self, key):
        if key == "type":
            return self.__class__.__name__
        try:
            return getattr(self, key)
        except AttributeError as e:
            raise KeyError(key) from e

    def __setitem__(self, key, value):
        assert key != "type"
        setattr(self, key, value)

    def __delitem__(self, key):
        assert 0

    def __iter__(self):
        yield "type"
        for attrib in attr.fields(self.__class__):
            yield attrib.name

    def __len__(self):
        return len(attr.fields(self.__class__)) + 1


@attr.s(slots=True, kw_only=True)
class Doctype(Token):
    name: str = attr.ib()
    publicId: Optional[str] = attr.ib()
    systemId: Optional[str] = attr.ib()


@attr.s(slots=True, kw_only=True)
class Characters(Token):
    data: str = attr.ib()


@attr.s(slots=True, kw_only=True)
class SpaceCharacters(Token):
    data: str = attr.ib()


@attr.s(slots=True, kw_only=True)
class Entity(Token):
    name: str = attr.ib()


@attr.s(slots=True, kw_only=True)
class StartTag(Token):
    namespace: str = attr.ib()
    name: str = attr.ib()
    data: MutableMapping = attr.ib()


@attr.s(slots=True, kw_only=True)
class EndTag(Token):
    namespace: str = attr.ib()
    name: str = attr.ib()


@attr.s(slots=True, kw_only=True)
class EmptyTag(Token):
    namespace: str = attr.ib()
    name: str = attr.ib()
    data: dict = attr.ib()


@attr.s(slots=True, kw_only=True)
class Comment(Token):
    data: str = attr.ib()


@attr.s(slots=True, kw_only=True)
class ParseError(Token):
    data: str = attr.ib()
    datavars: dict = attr.ib()


@attr.s(slots=True, kw_only=True)
class SerializeError(Token):
    #: Error message
    data: str = attr.ib()


# This is how the tokens generated by the tokenizer map to the token classes
# defined above.
#
# Notes:
#
# - The "type" field of tokenizer token dicts is an integer from
#   constants.tokenTypes, rather than the string "type" in the token dicts
#   generated by tree walkers.
#
# - Some of the tokenizer token dicts have extra keys (not in the classes
#   above) that are used internally by the tokenizer as it mutates the token to
#   its final form. For example, StarTag
#
# - The tokenizer resolves entities internally. It never generates Entity tokens.
#
# - The tokenizer may generate ParseError tokens, but not SerializeError tokens
#   (the latter comes from tree walkers).
#
_parse_token_types = {
    tokenTypes["Doctype"]: Doctype,
    tokenTypes["Characters"]: Characters,
    tokenTypes["SpaceCharacters"]: SpaceCharacters,
    tokenTypes["StartTag"]: StartTag,
    tokenTypes["EndTag"]: EndTag,
    tokenTypes["EmptyTag"]: EmptyTag,
    tokenTypes["Comment"]: Comment,
    tokenTypes["ParseError"]: ParseError,
}
